\section{Motivation}

Model-based systems engineering (MBSE) emerged to address a persistent limitation of document-centric systems development: critical system knowledge is fragmented across heterogeneous artifacts, updated at different cadences, and often only weakly coupled to verification and validation workflows~\cite{estefan2007mbseSurvey,madni2018mbse,sebokMBSE2025}. In document-first practice, requirements, architecture descriptions, interface definitions, analysis assumptions, and verification plans are frequently distributed across separate files and tools, which increases coordination overhead and weakens traceability under change. MBSE, by contrast, treats formal models as primary engineering artifacts, with documents increasingly becoming downstream views or reports derived from model state~\cite{incoseMbseInitiative2025,sebokMBSE2025,madni2018mbse}. This shift is not primarily stylistic; it is operational. It changes when defects are detectable, how cross-discipline dependencies are surfaced, and how consistently requirements-to-design-to-verification links can be maintained through the lifecycle~\cite{estefan2007mbseSurvey,madni2018mbse}.

Authoritative systems-engineering references have framed this transition as a core trajectory for the discipline rather than a niche method~\cite{incoseSEVision2035,incoseMbseInitiative2025,sebokMBSE2025}. INCOSEâ€™s MBSE initiative defines MBSE as the formalized application of modeling from conceptual design through later lifecycle phases, explicitly positioning models as replacements for document-oriented practices when scale and lifecycle continuity are required~\cite{incoseMbseInitiative2025}. SE Vision 2035 similarly positions model-based practice as central to digital transformation in systems engineering~\cite{incoseSEVision2035}. Independent MBSE scholarship converges on the same point: the value proposition is not only better representation but tighter integration of requirements analysis, architecture work, and verification activities under shared formal artifacts~\cite{madni2018mbse,estefan2007mbseSurvey}. The consequence is that model quality and model process quality become direct determinants of engineering throughput, rework load, and lifecycle risk posture.

This context is precisely why SysMLv2 matters. SysMLv2 is now standardized through OMG with a formal language specification, machine-readable normative artifacts, and explicit support for model interchange and tooling integration~\cite{omgSysMLv2Spec2024,omgSysMLv2About2025}. The standardization package includes normative and informative machine-readable documents (including XMI, JSON schema, and SysML textual artifacts), which materially lowers ambiguity for tool implementers and increases the feasibility of deterministic processing pipelines~\cite{omgSysMLv2About2025}. In parallel, the final-adoption materials emphasize complementary textual and graphical representations over one underlying model plus an API/services layer for navigation, query, and update across lifecycle tools~\cite{omgSysMLv2FinalAdoption2025}. That combination is important for motivation because it turns system modeling from a largely editor-bound activity into a programmatically accessible, toolchain-enforced process.

The practical implication is straightforward: once modeling languages are formal and textual, they become scriptable, testable, and machine-generable in the same broad sense that programming languages are scriptable, testable, and machine-generable~\cite{omgSysMLv2Spec2024,omgSysMLv2About2025}. This does not imply semantic correctness by default; it implies that syntactic and structural conformance can be evaluated deterministically by tools, and that generation systems can target those constraints directly. In other words, SysMLv2 moves part of systems modeling into the same regime that made large-scale automation viable in software engineering: formal syntax, deterministic validators, and reproducible interfaces to downstream tooling.

This shift creates an immediate opportunity for LLM-assisted generation. Large language models operate over text; they are most operationally useful when outputs can be checked against explicit machine constraints~\cite{peng2023copilot,ziegler2022productivityAssessment}. As SysMLv2 formalizes textual notation and model interchange representations, natural-language-to-model generation is no longer purely a prototyping concept; it is technically aligned with an executable toolchain context~\cite{omgSysMLv2Spec2024,omgSysMLv2About2025,omgSysMLv2FinalAdoption2025}. The relevant motivation question therefore becomes not whether language models can produce plausible SysML-like text, but whether they can reliably produce tool-accepted artifacts at rates that reduce lifecycle friction. That framing is intentionally conservative: it focuses on measurable conformance and workflow throughput rather than speculative claims about automated architecture judgment.

The strongest empirical prior for this framing comes from software engineering, where the impact of LLM assistants has now been studied across controlled experiments, field deployments, and randomized trials. A controlled GitHub Copilot experiment reported a 55.8\% reduction in completion time for a standardized programming task (71.17 minutes versus 160.89 minutes; 95\% CI for speedup 21\%--89\%) without a statistically significant difference in task success rates~\cite{peng2023copilot}. This result established a credible upper-bound scenario for short-horizon acceleration when generated output can be immediately checked in a familiar tool/test context. Subsequent field evidence, however, showed that effects are heterogeneous and context dependent. In multi-company randomized deployments, usage of coding assistants was associated with an estimated 26.08\% increase in weekly completed tasks and a 38.38\% increase in compilation events, indicating not merely faster typing but denser iterate-check cycles~\cite{cui2025highSkilled}. Related field analyses reported positive but uncertain pull-request throughput changes in the approximate 7.5\%--21.8\% range depending on setting and estimation strategy~\cite{demirer2024fieldCopilot}. 

Critically, this evidence is not uniformly positive. A randomized controlled trial with experienced open-source maintainers on mature repositories reported that AI-assisted conditions took 19\% longer on average, despite participants forecasting substantial speedups ex ante~\cite{becker2025metr}. The same broader literature finds mixed outcomes in collaborative and long-horizon development contexts, including situations where productivity improvements coexist with higher integration burden~\cite{song2024copilotOss}. For example, project-level OSS evidence reported a 6.5\% productivity increase together with a 41.6\% increase in integration time, highlighting that local code-generation gains can be partially offset by downstream coordination and review costs~\cite{song2024copilotOss}. The motivation signal is therefore nuanced but robust: automation can yield material gains, yet gains are mediated by project maturity, quality bar, context requirements, and validation overhead.

A second stable finding from software studies is a workflow reallocation from pure authoring toward validation and repair. Observational and qualitative work on real developer interactions with code assistants consistently describes alternating ``acceleration'' and ``exploration'' modes and documents substantial effort devoted to checking, adapting, and integrating suggested code~\cite{barke2023groundedCopilot,vaithilingam2022expectationExperience}. Controlled and mixed-method studies similarly report that perceived usefulness often persists even when objective speed gains are modest or inconsistent, because assistants can provide useful starting points while shifting burden to correctness assessment and refinement~\cite{vaithilingam2022expectationExperience,ziegler2022productivityAssessment}. Robustness studies strengthen this interpretation: semantically equivalent prompt changes can produce materially different outputs, increasing verification burden and reducing predictability of first-pass acceptance~\cite{mastropaolo2023robustness}. 

Taken together, these findings motivate a specific claim relevant to systems engineering: productivity improvements are most plausible when generated artifacts are usable within existing toolchains, and when the path from generated proposal to accepted artifact is short, observable, and repeatable~\cite{peng2023copilot,cui2025highSkilled,becker2025metr}. In software, that usability threshold is often approximated by compilation, testing, and integration readiness. In MBSE, the analogous threshold is toolchain-accepted model conformance under the language and platform constraints that govern downstream modeling, analysis, and verification tasks~\cite{omgSysMLv2Spec2024,omgSysMLv2FinalAdoption2025,sebokMBSE2025}. This analogy does not assume equal effect sizes across domains; it identifies a shared mechanism: when formal artifacts can be generated and checked against deterministic gates, iteration friction can decrease.

That mechanism creates a strong but bounded opportunity in SysMLv2-based engineering. If natural-language-driven generation can produce models that pass toolchain acceptance at high reliability, teams may reduce time spent on mechanical syntax repair, namespace and structure cleanup, and other low-leverage rework that does not advance architectural reasoning~\cite{omgSysMLv2Spec2024,omgSysMLv2About2025}. Faster arrival at tool-accepted baselines can shorten the interval between intent expression and model-based review, enabling more frequent architecture-level decision cycles in the same calendar time. This is conceptually parallel to software settings where increased compile/test cadence contributes to throughput gains~\cite{cui2025highSkilled,peng2023copilot}. 

At the same time, literature-informed caution is essential. First, tool acceptance is a necessary but insufficient condition for engineering adequacy; syntactic validity does not guarantee semantic correctness, requirement satisfaction, or acceptable safety and performance properties~\cite{sebokMBSE2025,madni2018mbse}. Second, heterogeneous software results indicate that integration and review costs can absorb or reverse local generation gains in high-standard contexts~\cite{becker2025metr,song2024copilotOss}. Third, robustness limits in generated outputs imply that operational deployment must account for variability and verification workload rather than presuming deterministic first-pass quality~\cite{mastropaolo2023robustness,vaithilingam2022expectationExperience}. For MBSE, this means the relevant motivation is not ``LLMs replace systems engineers,'' but ``formal, toolchain-accepted generation may reduce mechanical modeling friction and reallocate effort toward higher-value systems reasoning, provided validation disciplines remain explicit.''

Under this framing, SysMLv2 represents a timely inflection point for empirical study. The field now has a formal textual language standard, machine-readable specification artifacts, and a standardized API/services ecosystem signal~\cite{omgSysMLv2Spec2024,omgSysMLv2About2025,omgSysMLv2FinalAdoption2025}. Concurrently, software engineering has produced a sufficiently mature evidence base to set realistic expectations: meaningful gains are possible, effects are heterogeneous, and validator-mediated workflows are central to practical impact~\cite{peng2023copilot,cui2025highSkilled,becker2025metr,barke2023groundedCopilot}. The resulting motivation for toolchain-gated SysMLv2 generation is therefore evidence-backed and conservative: if generated models can reliably satisfy deterministic toolchain gates, MBSE teams may compress low-value iteration loops and spend proportionally more effort on requirements interpretation, architectural trade studies, and verification intent. Whether and where this occurs is an empirical question, but the preconditions for testing it now exist.

