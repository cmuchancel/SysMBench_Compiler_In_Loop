\section{State of the Art}

Recent work on natural-language--to--SysMLv2 generation has moved from proof-of-concept prompting toward structured, validation-centric pipelines that explicitly target syntactic reliability under formal language constraints. Three threads are now central in this space: multi-agent and template-mediated generation systems, parser-gated repair frameworks, and benchmark-driven evaluation of generated model quality~\cite{bouamra2025systemp,cibrian2025sysmlagent,jin2025sysmbench}. Taken together, these efforts define the current state of the art: generation is no longer treated as a single decoding step, but as an iterative modeling workflow in which generated artifacts are repeatedly checked against formal representations of SysMLv2 syntax and revised when violations are detected.

A first representative line of work is SysTemp, which frames SysMLv2 generation as a coordinated multi-agent process rather than a single-model pass~\cite{bouamra2025systemp}. Its architecture combines requirement extraction, template-driven structure synthesis, and parser-based feedback to improve syntactic regularity of generated outputs. The central contribution is not merely prompt engineering but decomposition of the modeling task into stages that reduce unconstrained generation. By introducing templates before free-form completion and routing outputs through grammar-level checks, SysTemp operationalizes a design principle that is increasingly visible across the literature: constrain early, validate often, repair iteratively. In reported experiments, this design improves convergence toward parseable outputs relative to unconstrained one-shot baselines, and it therefore constitutes an important baseline in contemporary SysMLv2 generation work~\cite{bouamra2025systemp}.

A second representative effort is the 2025 \emph{Computers in Industry} agent-based framework by Cibri\'an \emph{et al.}, which combines retrieval-augmented generation with ANTLR-backed grammar validation~\cite{cibrian2025sysmlagent}. This work reports strong syntactic validity on curated prompt sets under grammar-level acceptance criteria and further reinforces the broader methodological trend toward explicit validation loops. Its contribution is especially relevant because it links retrieval, agent orchestration, and formal parsing in a single pipeline, illustrating how model generation can be grounded in explicit language constraints rather than solely in latent model priors. Together with SysTemp, it establishes parser-mediated iterative correction as a practical, reproducible design strategy for SysMLv2 synthesis in sparse-data settings~\cite{cibrian2025sysmlagent,bouamra2025systemp}.

A third pillar of the state of the art is SysMBench, which provides a benchmark and evaluation framework for natural-language--to--SysML generation~\cite{jin2025sysmbench}. SysMBench does not primarily propose a new parser architecture; instead, it creates a common evaluation substrate for comparing generation methods, prompt strategies, and model families. This benchmark-centric contribution is significant because it shifts discussion from anecdotal examples to repeatable, prompt-level measurement and enables more rigorous comparison of quality dimensions across systems~\cite{jin2025sysmbench}. In current literature, SysMBench often serves as the empirical backbone for reporting progress, especially when evaluating semantic alignment metrics rather than parser-only acceptance.

Despite differences in agent design and prompting detail, successful systems in this literature share a common architectural pattern. First, retrieval-augmented generation (RAG) is used to inject language references, domain fragments, or exemplar snippets into generation context, reducing unsupported token sequences and improving alignment with expected constructs~\cite{cibrian2025sysmlagent,bouamra2025systemp}. Second, template or skeleton construction is used to anchor global model structure before local completion, which narrows the space of admissible outputs and reduces high-variance free-form synthesis~\cite{bouamra2025systemp}. Third, grammar-based parsing acts as a formal gate, rejecting outputs that violate declared syntax. Fourth, failed outputs are routed into iterative repair, where diagnostics (or parse-failure traces) condition subsequent revisions. This yields a canonical pipeline shape that can be summarized as \emph{Generate $\rightarrow$ Parse $\rightarrow$ Repair $\rightarrow$ Repeat}~\cite{bouamra2025systemp,cibrian2025sysmlagent,wang2022compilable,grubisic2024compiler}.

The importance of this pattern is methodological as much as empirical. It externalizes correctness signals from opaque model behavior into deterministic validators. In other words, syntactic acceptance is delegated to formal artifacts (grammars/parsers), while the LLM is used primarily as a proposal engine. This separation is now common across both SysML-oriented work and compiler-feedback code-generation literature more broadly, where iterative validation loops have repeatedly outperformed unconstrained one-shot generation in compilability-oriented settings~\cite{wang2022compilable,grubisic2024compiler}. Within SysMLv2 generation specifically, the same design logic appears in agentic and template-based systems, even when the validation substrate differs (ANTLR grammars, Xtext grammars, or tool-specific parsers).

A precise understanding of grammar-based validation is therefore essential for interpreting current claims. ANTLR is a parser generator that takes a declarative grammar specification and produces lexer/parser implementations for target languages~\cite{parr2013antlr4}. The grammar formalism used in this context is fundamentally context-free: it specifies admissible token sequences and hierarchical phrase structure through production rules. Parsing under this regime determines whether an input string belongs to the language defined by the grammar, and successful parsing indicates syntactic well-formedness relative to that grammar. In compiler terminology, this is syntax analysis: tokenization plus derivation into parse trees or equivalent abstract syntax structures~\cite{aho2006compilers,parr2013antlr4}.

However, syntax analysis is only one phase of language validation. Context-free parsing cannot, by itself, enforce context-sensitive constraints that depend on symbol environments, typing contexts, ownership relations, or cross-reference consistency~\cite{aho2006compilers,nielson2015principles}. Those checks belong to semantic analysis, where implementations maintain symbol tables, resolve identifiers to declarations under lexical or model scopes, propagate and verify type information, and evaluate static semantic rules that exceed grammar expressiveness. In modeling languages, additional constraints often include multiplicity validity, consistency of redefinitions, conformance of specialized elements, and validity of cross-model references. These are static semantic obligations, not mere parse obligations.

This distinction matters directly for SysMLv2 generation reports that use grammar acceptance as a primary syntactic endpoint. Parser success guarantees structural conformance to the declared context-free grammar, but it does not guarantee that the produced artifact is acceptable to a full production toolchain that also performs name resolution, type checking, semantic constraint enforcement, and sometimes profile- or tool-specific static checks~\cite{omgSysMLv2Spec2024,omgSysMLv2About2025,sensmetry2024syside}. Put differently, parser validity is necessary for downstream acceptance but not sufficient for it. This is not a criticism of grammar-based systems; it is a boundary condition of what parser-based validation is designed to certify.

The same framing applies to Xtext-based and ANTLR-based pipelines more generally. Xtext and ANTLR can both provide strong grammar-level guarantees and robust diagnostics at the syntax boundary, and both can be extended with semantic validators. Yet many generation studies report parser-level outcomes more prominently than full toolchain acceptance under production compilers or integrated modeling environments~\cite{cibrian2025sysmlagent,bouamra2025systemp}. As a result, comparisons across papers can mix different correctness oracles: one system may report grammar acceptance, another may report richer static checks, and a third may report downstream task-level scores. Without explicit oracle alignment, ``syntactic validity'' can refer to materially different validation depth.

SysMBench is particularly important in clarifying this measurement landscape. Its core contribution is benchmark construction and evaluation protocol for natural-language-driven system model generation, with emphasis on semantic matching through metrics such as SysMEval-F1~\cite{jin2025sysmbench}. In that role, SysMBench helps quantify how well generated models preserve intended meaning or reference alignment under benchmark scoring procedures. This is a substantial advance over purely anecdotal evaluation and provides a common substrate for semantic-comparison experiments.

At the same time, SysMBench is not primarily a syntactic compilability benchmark in the narrow toolchain-acceptance sense. Its headline objective is semantic evaluation quality, not parser-versus-compiler acceptance accounting per se~\cite{jin2025sysmbench}. Consequently, it can reveal trade-offs in prompting and generation strategy where output becomes closer to benchmark references under semantic scoring, while syntactic/toolchain acceptance behavior may require separate accounting under a specific validation oracle. This distinction is analytically useful: semantic similarity and syntactic acceptance are related but non-identical dimensions, and optimizing one does not automatically optimize the other.

Recent benchmark analyses also suggest that grammar prompting and structure-constrained prompting can alter the balance between semantic faithfulness and surface conformance. In practical terms, stronger scaffolding may reduce malformed output and improve structural regularity, while simultaneously introducing template bias or lexical anchoring effects that influence semantic precision metrics in nontrivial ways~\cite{jin2025sysmbench,bouamra2025systemp,cibrian2025sysmlagent}. For state-of-the-art synthesis, the key point is not that one objective dominates universally, but that current systems are multi-objective and must often trade syntactic regularity, semantic alignment, and generation flexibility under finite context and limited domain exemplars.

Viewed as a whole, the present state of the art achieves syntactic reliability primarily through three coupled mechanisms: grammar-based parser gates (ANTLR/Xtext-style), retrieval grounding, and iterative validation-repair loops~\cite{bouamra2025systemp,cibrian2025sysmlagent,wang2022compilable,grubisic2024compiler}. These mechanisms have clearly improved robustness relative to unconstrained one-pass generation and have made SysMLv2 generation pipelines materially more reproducible. They also provide a coherent engineering template for future systems: explicit structure priors, deterministic validation checkpoints, and iterative correction under failure.

Nonetheless, several boundaries remain visible in current literature. First, validation oracles are still frequently parser-centric, whereas production modeling environments often apply richer static semantics beyond grammar acceptance~\cite{omgSysMLv2Spec2024,sensmetry2024syside}. Second, generation-time guarantees are rare: most systems report empirical convergence or observed validity rates rather than formal guarantees tied to specific acceptance definitions~\cite{bouamra2025systemp,cibrian2025sysmlagent}. Third, semantic correctness remains partially unresolved even when syntax quality improves, as reflected by the continuing need for benchmark-level semantic metrics and prompt-strategy sensitivity analyses~\cite{jin2025sysmbench}. These observations do not diminish current progress; rather, they define the technical frontier that the next wave of SysMLv2 generation research is now addressing.
